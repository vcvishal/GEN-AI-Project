{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17080678-9d20-40c0-a21c-ed997145ca62",
   "metadata": {},
   "source": [
    "WhatEver I Understood\n",
    "\n",
    "\n",
    "\n",
    "Compulsory Goals:\n",
    "Dataset >>>>Done\n",
    "\n",
    "\n",
    "Load the files and index them in a vector database  >>Done\n",
    "\n",
    "\n",
    "Experiment with different embedding models (open-source from huggingface and commercial ones like OpenAI) >> Done\n",
    "\n",
    "\n",
    "Experiment with various retrieval strategies (simple cosine to hybrid search and rerankers) >>>>> Multiquery,Similarity,Reranker\n",
    "\n",
    "\n",
    "Test the RAG pipeline on sample queries >> image added\n",
    "\n",
    "\n",
    "Try to also show the source of the generated response (which context documents were used to generate the response - top 3 will do)> done\n",
    "\n",
    "\n",
    "Advanced Option : Build a streamlit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f845c569-138f-40ac-b76e-9519d7740391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting app_again.py\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%%writefile app_again.py\n",
    "\n",
    "#importing Libraries\n",
    "import os\n",
    "os.environ['OPENAI_API_KEY'] = use your key\n",
    "HUGGINGFACEHUB_API_TOKEN = use you key\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = HUGGINGFACEHUB_API_TOKEN\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain.retrievers.document_compressors import LLMChainFilter\n",
    " \n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_community.chat_message_histories import StreamlitChatMessageHistory\n",
    "from langchain_core.callbacks.base import BaseCallbackHandler\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores.chroma import Chroma\n",
    "from operator import itemgetter\n",
    "import streamlit as st\n",
    "import tempfile\n",
    "from langchain_community.cross_encoders import HuggingFaceCrossEncoder\n",
    "from langchain.retrievers.document_compressors import CrossEncoderReranker\n",
    "import pandas as pd\n",
    "from langchain_community.llms import HuggingFaceHub\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# # HuggingFace model setup\n",
    "repo_id = \"google/flan-t5-xxl\"\n",
    "#hf_model = HuggingFaceHub(repo_id=repo_id, model_kwargs={\"temperature\": 0.1, \"max_length\": 64})\n",
    "chatgpt = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0.1,\n",
    "                     streaming=True)\n",
    "st.set_page_config(page_title=\"Chatbot\", page_icon=\"ðŸ¤–\")\n",
    "st.title(\"Welcome to Question Answer RAG Chatbot ðŸ¤–\")\n",
    "\n",
    "\n",
    "class HuggingFaceEmbeddingsWrapper:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def embed_documents(self, texts):\n",
    "        \"\"\"Embed a list of documents.\"\"\"\n",
    "        return self.model.encode(texts, convert_to_tensor=False).tolist()\n",
    "\n",
    "    def embed_query(self, query):\n",
    "        \"\"\"Embed a single query.\"\"\"\n",
    "        return self.model.encode([query], convert_to_tensor=False).tolist()[0]\n",
    "\n",
    "# Initialize the embedding wrapper\n",
    "\n",
    "embedding_model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "sentence_model = SentenceTransformer(embedding_model_name)\n",
    "huggingface_embeddings = HuggingFaceEmbeddingsWrapper(sentence_model)\n",
    "\n",
    "# Folder containing PDF files\n",
    "folder_path = \"dataset\"\n",
    "\n",
    "# Load documents from PDFs\n",
    "def load_documents(folder_path):\n",
    "    doc_list = []\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith(\".pdf\"):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            loader = PyMuPDFLoader(file_path)\n",
    "            docs = loader.load()\n",
    "            doc_list.extend(docs)\n",
    "    return doc_list\n",
    "doc_list=load_documents(folder_path)\n",
    "\n",
    "#retrivers \n",
    "def create_openai_retriever(doc_list):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=200)\n",
    "    doc_chunks = text_splitter.split_documents(doc_list)\n",
    "    embeddings_model = OpenAIEmbeddings()\n",
    "    vectordb = Chroma.from_documents(doc_chunks, embeddings_model, collection_name=\"openai_embeddings\")\n",
    "    return vectordb.as_retriever()\n",
    "\n",
    "\n",
    "def create_huggingface_retriever(doc_list):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=200)\n",
    "    doc_chunks = text_splitter.split_documents(doc_list)\n",
    "    vectordb = Chroma.from_documents(doc_chunks, huggingface_embeddings, collection_name=\"hf_embeddings\")\n",
    "    return vectordb.as_retriever()\n",
    "\n",
    "retriever_hg=create_huggingface_retriever(doc_list)\n",
    "retriever_oi=create_openai_retriever(doc_list)\n",
    "# #print(doc_list)\n",
    "\n",
    "### Similarity or Ranking based Retrieval\n",
    "\n",
    "def create_openai_retriever_sim(doc_list):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=200)\n",
    "    doc_chunks = text_splitter.split_documents(doc_list)\n",
    "    embeddings_model = OpenAIEmbeddings()\n",
    "    vectordb = Chroma.from_documents(doc_chunks, embeddings_model, collection_name=\"openai_embeddings\")\n",
    "    return vectordb.as_retriever(search_type=\"similarity\",search_kwargs={\"k\": 3})\n",
    "\n",
    "\n",
    "def create_huggingface_retriever_sim(doc_list):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=200)\n",
    "    doc_chunks = text_splitter.split_documents(doc_list)\n",
    "    vectordb = Chroma.from_documents(doc_chunks, huggingface_embeddings, collection_name=\"hf_embeddings\")\n",
    "    return vectordb.as_retriever(search_type=\"similarity\",search_kwargs={\"k\": 3})\n",
    "retriever_hg=create_openai_retriever_sim(doc_list)\n",
    "retriever_oi=create_huggingface_retriever_sim(doc_list)\n",
    "\n",
    "##  Multiquery\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "\n",
    "def create_huggingface_retriever_mq(doc_list):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=200)\n",
    "    doc_chunks = text_splitter.split_documents(doc_list)\n",
    "    vectordb = Chroma.from_documents(doc_chunks, huggingface_embeddings, collection_name=\"hf_embeddings\")\n",
    "    sm=vectordb.as_retriever(search_type=\"similarity\",search_kwargs={\"k\": 3})\n",
    "    return MultiQueryRetriever.from_llm(\n",
    "    retriever=sm, llm=chatgpt\n",
    ")\n",
    "\n",
    "def create_openai_retriever_mq(doc_list):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=200)\n",
    "    doc_chunks = text_splitter.split_documents(doc_list)\n",
    "    embeddings_model = OpenAIEmbeddings()\n",
    "    vectordb = Chroma.from_documents(doc_chunks, embeddings_model, collection_name=\"openai_embeddings\")\n",
    "    sm=vectordb.as_retriever(search_type=\"similarity\",search_kwargs={\"k\": 3})\n",
    "    return MultiQueryRetriever.from_llm(\n",
    "    retriever=sm, llm=chatgpt\n",
    ")\n",
    "\n",
    "retriever_oi=create_openai_retriever_mq(doc_list)\n",
    "retriever_hg=create_huggingface_retriever_mq(doc_list)\n",
    "\n",
    "\n",
    "\n",
    "##Reranker\n",
    "\n",
    "\n",
    "def create_openai_retriever_reranker(doc_list):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=200)\n",
    "    doc_chunks = text_splitter.split_documents(doc_list)\n",
    "    embeddings_model = OpenAIEmbeddings()\n",
    "    vectordb = Chroma.from_documents(doc_chunks, embeddings_model, collection_name=\"openai_embeddings\")\n",
    "    sm=vectordb.as_retriever(search_type=\"similarity\",search_kwargs={\"k\": 3})\n",
    "    _filter = LLMChainFilter.from_llm(llm=chatgpt)\n",
    "    # Retriever 2 - retrieves the documents similar to query and then applies the filter\n",
    "    compressor_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=_filter, base_retriever=sm\n",
    "    )\n",
    "    reranker = HuggingFaceCrossEncoder(model_name=\"BAAI/bge-reranker-large\")\n",
    "    reranker_compressor = CrossEncoderReranker(model=reranker, top_n=3)\n",
    "    return ContextualCompressionRetriever(\n",
    "    base_compressor=reranker_compressor, base_retriever=compressor_retriever\n",
    ")\n",
    "retriever_hg=create_openai_retriever_reranker(doc_list)\n",
    "\n",
    "if not doc_list:\n",
    "    st.error(\"No PDF documents found in the 'dataset' folder.\")\n",
    "else:\n",
    "    st.success(f\"Loaded {len(doc_list)} documents.\")\n",
    "\n",
    "class StreamHandler(BaseCallbackHandler):\n",
    "  def __init__(self, container, initial_text=\"\"):\n",
    "    self.container = container\n",
    "    self.text = initial_text\n",
    "\n",
    "  def on_llm_new_token(self, token: str, **kwargs) -> None:\n",
    "    self.text += token\n",
    "    self.container.markdown(self.text)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Load a connection to ChatGPT LLM\n",
    "chatgpt = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0.1,\n",
    "                     streaming=True)\n",
    "\n",
    "# Create a prompt template for QA RAG System\n",
    "qa_template = \"\"\"\n",
    "              Use only the following pieces of context to answer the question at the end.\n",
    "              If you don't know the answer, just say that you don't know,\n",
    "              don't try to make up an answer. Keep the answer as concise as possible.\n",
    "\n",
    "              {context}\n",
    "\n",
    "              Question: {question}\n",
    "              \"\"\"\n",
    "qa_prompt = ChatPromptTemplate.from_template(qa_template)\n",
    "\n",
    "# This function formats retrieved documents before sending to LLM\n",
    "def format_docs(docs):\n",
    "  return \"\\n\\n\".join([d.page_content for d in docs])\n",
    "\n",
    "# Create a QA RAG System Chain\n",
    "qa_rag_chain_oi = (\n",
    "  {\n",
    "    \"context\": itemgetter(\"question\") # based on the user question get context docs\n",
    "      |\n",
    "    retriever_oi\n",
    "      |\n",
    "    format_docs,\n",
    "    \"question\": itemgetter(\"question\") # user question\n",
    "  }\n",
    "    |\n",
    "  qa_prompt # prompt with above user question and context\n",
    "    |\n",
    "  chatgpt # above prompt is sent to the LLM for response\n",
    ")\n",
    "\n",
    "qa_rag_chain_hg = (\n",
    "  {\n",
    "    \"context\": itemgetter(\"question\") # based on the user question get context docs\n",
    "      |\n",
    "    retriever_hg\n",
    "      |\n",
    "    format_docs,\n",
    "    \"question\": itemgetter(\"question\") # user question\n",
    "  }\n",
    "    |\n",
    "  qa_prompt # prompt with above user question and context\n",
    "    |\n",
    "  chatgpt # above prompt is sent to the LLM for response\n",
    ")\n",
    "\n",
    "# Store conversation history in Streamlit session state\n",
    "streamlit_msg_history = StreamlitChatMessageHistory(key=\"langchain_messages\")\n",
    "\n",
    "# Shows the first message when app starts\n",
    "if len(streamlit_msg_history.messages) == 0:\n",
    "  streamlit_msg_history.add_ai_message(\"Please ask your question?\")\n",
    "\n",
    "# Render current messages from StreamlitChatMessageHistory\n",
    "for msg in streamlit_msg_history.messages:\n",
    "  st.chat_message(msg.type).write(msg.content)\n",
    "\n",
    "\n",
    "class PostMessageHandler(BaseCallbackHandler):\n",
    "  def __init__(self, msg: st.write):\n",
    "    BaseCallbackHandler.__init__(self)\n",
    "    self.msg = msg\n",
    "    self.sources = []\n",
    "\n",
    "  def on_retriever_end(self, documents, *, run_id, parent_run_id, **kwargs):\n",
    "    source_ids = []\n",
    "    for d in documents: # retrieved documents from retriever based on user query\n",
    "      metadata = {\n",
    "        \"source\": d.metadata[\"source\"],\n",
    "        \"page\": d.metadata[\"page\"],\n",
    "        \"content\": d.page_content[:200]\n",
    "      }\n",
    "      idx = (metadata[\"source\"], metadata[\"page\"])\n",
    "      if idx not in source_ids: # store unique source documents\n",
    "        source_ids.append(idx)\n",
    "        self.sources.append(metadata)\n",
    "\n",
    "  def on_llm_end(self, response, *, run_id, parent_run_id, **kwargs):\n",
    "    if len(self.sources):\n",
    "      st.markdown(\"__Sources:__ \"+\"\\n\")\n",
    "      st.dataframe(data=pd.DataFrame(self.sources[:3]),\n",
    "                    width=1000) # Top 3 sources\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if user_prompt := st.chat_input():\n",
    "    # Display the user's input\n",
    "    st.chat_message(\"human\").write(user_prompt)\n",
    "\n",
    "    # OpenAI Response Handling\n",
    "    with st.chat_message(\"ai\"):\n",
    "        stream_handler = StreamHandler(st.empty())\n",
    "        sources_container = st.empty()  # Dynamic container for sources\n",
    "        pm_handler = PostMessageHandler(sources_container)\n",
    "\n",
    "        config = {\n",
    "            \"configurable\": {\"session_id\": \"any\"},\n",
    "            \"callbacks\": [stream_handler, pm_handler]\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            response_oi = qa_rag_chain_oi.invoke({\n",
    "                \"question\": user_prompt,\n",
    "                \"context\": lambda question: format_docs(retriever.get_relevant_documents(question)),\n",
    "            }, config)\n",
    "\n",
    "\n",
    "        except Exception as e:\n",
    "            st.error(f\"An error occurred with OpenAI chain: {e}\")\n",
    "\n",
    "    # HuggingFace Response Handling\n",
    "    with st.chat_message(\"ai\"):\n",
    "        stream_handler = StreamHandler(st.empty())\n",
    "        sources_container = st.empty()\n",
    "        pm_handler = PostMessageHandler(sources_container)\n",
    "\n",
    "        config = {\n",
    "            \"configurable\": {\"session_id\": \"any\"},\n",
    "            \"callbacks\": [stream_handler, pm_handler]\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            response_hg = qa_rag_chain_hg.invoke({\"question\": user_prompt}, config)\n",
    "\n",
    "\n",
    "\n",
    "        except Exception as e:\n",
    "            st.error(f\"An error occurred with HuggingFace chain: {e}\")\n",
    "\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3179b67c-956b-42e6-9211-12bfe9151975",
   "metadata": {},
   "source": [
    "## THANK YOU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5022caa0-15e2-4bd7-8c22-a611cb8331d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
